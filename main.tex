\documentclass{article}

\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{algpseudocode}

\title{Secure Aggregation with Global Differential Privacy for Federated Machine Learning with Malicious Clients}
\author{Olivia Roehrig  \\
	Your Company / University  \\
	\and
	Maxim Urschumzew \\
	His Company / University \\
	}

\date{\today}
% Hint: \title{what ever}, \author{who care} and \date{when ever} could stand 
% before or after the \begin{document} command 
% BUT the \maketitle command MUST come AFTER the \begin{document} command! 
\begin{document}

\maketitle


\begin{abstract}
Short introduction to subject of the paper \ldots 
\end{abstract}

\section{Introduction}
Data privacy is one of the factors motivating federated learning. A machine learning model can be trained locally on computers owned by data owners (clients), who exchange only intermediate training results (gradients), instead of the data itself, with a central server that aggregates them into a shared model~\cite{McMahan2016CommunicationEfficientLO}. While this relieves the need to collect possibly sensitive data in a central location to perform the training process, the shared model and the gradients themselves contain enough information to enable reconstruction of private information~\cite{7958568}\cite{Boenisch2021WhenTC}.

A solution to this problem is using a modified training algorithm that provides \emph{differential privacy} for the model~\cite{Abadi_2016}. The technique adds a small amount of noise to the result of each training step, calibrated in a way that makes high certainty statements about the presence of an individual point in the training dataset unlikely to be inferrable from the model.

In this document, we present an architecture that allows using federated gradient descent while providing differential privacy for the training result as well as all intermediate gradients.

Adding noise locally will require a large amount of noise and hence large utility loss to achieve privacy, so our protocol adds noise after aggregating intermediate results (\emph{global} differential privacy). To avoid revealing the non-noised aggregate to even the party adding the noise, we use the Prio protocol for \emph{secure aggregation}~\cite{prio}.

\paragraph{Threat model and privacy guarantees.}
Our system requires two non-colluding aggregation servers that can be curious but are honestly executing the protocol. All other participants, namely the clients and a server coordinating federated model updates, can be malicious. In this setting, we can guarantee anonymity (no adversary can tell which client submitted which data value) and privacy (no adversary learns anything about any honest clients' data values except the differentially private aggregate) as well as differential privacy for all information exchanged between participants as well as the final training result.

In addition, the system is robust towards data poisoning attacks by limiting the influence a single client can have on the aggregation result.

\paragraph{Related work.}
There is a variety of work on federated learning with secure aggregation and global differential privacy. Systems that provide both differ from ours by the assumed threat model:
\begin{itemize}
\item\cite{dprio} no input validation, only one out of at least two servers needs to be honest but curious, but allows only small fraction of dishonest clients
\item\cite{Stevens2021EfficientDP} no input validation, server may be malicious, but requires honest majority of clients
\item\cite{Kairouz2021TheDD} no input validation, server may be malicious, but all clients need to be honest (which is realistic when having access to trusted execution environments)
\item\cite{acorn} can do input validation, only one out of at least two servers needs to be honest but curious, but allows only small fraction of dishonest clients
\end{itemize} \section{Variants of privacy}

\paragraph{Explanations}

\paragraph{Secure aggregation}

\paragraph{differential privacy}

\section{Our system}

\paragraph{building blocks}

\paragraph{the pseudo code}

\begin{algorithm}
  \caption{Client procedure}\label{client}

  \begin{algorithmic}[1]
  \State\textbf{Input:} Training dataset $X$, loss function $\mathcal L(\theta; x)$, number of rounds $N$, fixed point encoding bit length $b$
  \State\textbf{Output:} Model $\theta$ trained on dataset $X$ for $N$ rounds

  \For{$N$}
  \State$\theta$ = retrieve current shared model from the ML server server
  \State\textbf{G} = $\frac{1}{|X|} \sum_{x\in X} \nabla\mathcal L(\theta; x)$ (model gradient average)
  \State\textbf{G} = $\textbf{G}/\mathbf{max}\{1,||\textbf{G}||_{L_2}\}$ (clip $G$ to $L_2$ norm 1)
  \State\textbf{G} = round entries towards $0$ to get $b$-bit fixed point number in $(-1, 1)$
  \State\textbf{G} = apply $p(x) = 2^{b-1}\cdot x + 2^{b-1}$ to all entries to get an integer vector
  \State send one secret share of \textbf{G} to each of the aggregation servers
  \EndFor
  \State$\theta$ = retrieve current shared model from the ML server
  \State\Return $\theta$
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}
  \caption{Aggregator server procedure}\label{server}
  \begin{algorithmic}[1]
  \State \textbf{Input:} Set of clients $C$, privacy parameter $\rho$, fixed point encoding bit length $b$, finite field $F$ with order $p /geq 2^b$
  \State \textbf{Output:} Aggregate gradient, noised to be $\rho$-zero concentrated differentially private

  \State \textbf{G} = 0
  \For{all clients $\textbf{c} \in C$}
       \State \textbf{g} = retrieve gradient share from \textbf{c}
	   \State verify \textbf{g} is a share of a fixed point vector with $L_2$ norm $\leq 1$ projected to the field
	   \State \textbf{G} = \textbf{G} + \textbf{g}
  \EndFor
  \State \textbf{G} = \textbf{G} + $n$ where $n$ is drawn from the discrete Gaussian distribution $\mathcal N_\mathbb{Z}(0,\frac{2^n}{\sqrt{2\rho}})$
  \State \textbf{G} = $\textbf{G}\textnormal{ mod } p$ and map to field $F$
  \State retrieve aggregate share from other aggregation server and decrypt
  \State apply $pp(y) = 2^{1-b} \cdot y - |C|$ componentwise to obtain a float vector
  \end{algorithmic}
\end{algorithm}
\section{Privacy analysis}


\section{Utility analysis}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}

